{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinisilvanunes/P1PLN/blob/master/Aula_03_Processamento_de_Texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aula 03** - Processamento de Texto e Pr√©-processamento de Dados"
      ],
      "metadata": {
        "id": "T5dkw3AClodw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O pr√©-processamento limpa e transforma esse texto para facilitar o trabalho do algoritmo, deixando s√≥ as informa√ß√µes relevantes. T√©cnicas de Pr√©-processamento de Texto:\n",
        "\n",
        "1. **Normaliza√ß√£o de Texto** - Ajuste do texto para ter uma grafia padronizada;\n",
        "2. **Remo√ß√£o de Ruido** - Retirar elementos do texto que n√£o agregam valor √† an√°lise e podem atrapalhar;\n",
        "3. **Tokeniza√ß√£o** - Tokenizar √© dividir o texto em pequenas unidade;\n",
        "4. **Remo√ß√£o de Stopwords** - Remover palavras que n√£o carregam muito significado para an√°lise;\n",
        "5. **Stemming** - T√©cnica para reduzir palavras √†s suas raizes ou radicais, cortando sufixos e prefixos;\n",
        "6. **Lematiza√ß√£o** - Redu√ß√£o da palavra para a sua forma de dicion√°rio (forma can√¥nica)."
      ],
      "metadata": {
        "id": "9_EBpm9Bl0TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Normaliza√ß√£o de texto e Remo√ß√£o de Ruido"
      ],
      "metadata": {
        "id": "nQsZsmk5sbLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Remover caracteres especiais, pontua√ß√µes, e normalizar o uso de letras mai√∫sculas e min√∫sculas"
      ],
      "metadata": {
        "id": "iAEgn3D7sol7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importa a biblioteca para trabalhar com express√µes regulares\n",
        "import re\n",
        "\n",
        "original = \"Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\"\n",
        "\n",
        "texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\\s]', '',original)\n",
        "  # re.sub() fun√ß√£o que realiza substitui√ß√£o\n",
        "  # r'[^A-Za-z√Ä-√ø\\s]'>>> express√£o regular que define um conjunto de caracteres a serem removidos\n",
        "    # [A-Za-z√Ä-√ø\\s] >>> define um conjunto de caracteres de A at√© Z, a at√© z e acentos e espa√ßos\n",
        "    # ^faz a nega√ß√£o de uma express√£o regular\n",
        "  # '' substitui a express√£o regular por uma string vazia\n",
        "\n",
        "texto_normalizado = texto_limpo.lower()\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\nTexto normalizado: {texto_normalizado}')"
      ],
      "metadata": {
        "id": "MLcaRgmBtUSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokeniza√ß√£o"
      ],
      "metadata": {
        "id": "zPHxZp3XtWRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Tokeniza√ß√£o √© dividir o texto em unidades menores (tokens), que geralmente s√£o palavras"
      ],
      "metadata": {
        "id": "bVCrmZVDxPzS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxsds8nIe2rW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "tokens = word_tokenize(texto_normalizado)\n",
        "\n",
        "print(f'Texto original: {original}')\n",
        "print(f'\\n\\nTexto limpo: {texto_limpo}')\n",
        "print(f'\\n\\nTexto normalizado: {texto_normalizado}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remo√ß√£o de Stopwords"
      ],
      "metadata": {
        "id": "X0YPELSUxjL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Stopwords s√£o palavras de pouco valor sem√¢ntico (como \"de\", \"a\", \"o\") que podem ser removidas para simplificar o texto"
      ],
      "metadata": {
        "id": "hDAJIcohxnOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "\n",
        "print(stopwords_pt)\n",
        "\n",
        "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
        "\n",
        "print(f'\\n\\nTokens extraidos: {tokens} + \\n quantidade de tokens: {len(tokens)}')\n",
        "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords} + \\n quantidade de tokens: {len(tokens_sem_stopwords)}\\n')"
      ],
      "metadata": {
        "id": "MPCy16XexyWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Stemming e Lemaliza√ß√£o"
      ],
      "metadata": {
        "id": "ze4bxKBfx7WQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Stemming reduz as palavras √†s suas ra√≠zes (ou radicais);\n",
        "*   Lematiza√ß√£o normaliza as palavras para suas formas base, levando em conta contexto e gram√°tica."
      ],
      "metadata": {
        "id": "Iwn1attYx8mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RSLPStemmer\n",
        "\n",
        "nltk.download('rslp')\n",
        "\n",
        "stemmer = RSLPStemmer()\n",
        "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
        "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords}')\n",
        "print(f'\\n\\nTokens radicais: {stemming}\\n\\n\\n')"
      ],
      "metadata": {
        "id": "fAFQyx0gx_SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Exemplo 01 - Pr√© Processamento completo"
      ],
      "metadata": {
        "id": "9cKBfNLJyhzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Download dos recursos do NLTK (se necess√°rio)\n",
        "#nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Texto de exemplo\n",
        "texto = input(\"Insira um texto que seja coerente, podendo ter s√≠mbolos: \")\n",
        "\n",
        "# Limpeza de ru√≠dos e normaliza√ß√£o\n",
        "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)  # Remove tudo que n√£o for letra e substitui por espa√ßo\n",
        "texto_lower = texto_limpo.lower()  # Converte para min√∫sculas\n",
        "\n",
        "# Tokeniza√ß√£o\n",
        "tokens = nltk.word_tokenize(texto_lower)\n",
        "\n",
        "# Remo√ß√£o de stopwords\n",
        "stop_words = set(stopwords.words('portuguese'))\n",
        "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
        "\n",
        "# Impress√£o do resultado final\n",
        "print(palavras_stemizadas)"
      ],
      "metadata": {
        "id": "8zBXUooxyi_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 02 - Estrutura de Pr√©-processamento de texto"
      ],
      "metadata": {
        "id": "530IRn23yl7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "A1BG21ceyqJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "\n",
        "# Baixar stopwords do NLTK (se necess√°rio)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('rslp')\n",
        "\n",
        "# Carregar modelo do spaCy (portugu√™s como exemplo, pode trocar para 'en_core_web_sm' se for ingl√™s)\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Texto de exemplo (pode ser uma review ou trecho de not√≠cia)\n",
        "texto = \"O Processamento de Linguagem Natural (PLN) √© uma √°rea essencial da intelig√™ncia artificial! üòä Confira mais em: https://exemplo.com\"\n",
        "\n",
        "# 1. Normaliza√ß√£o (remover acentos, transformar em min√∫sculas, etc.)\n",
        "def normalizar_texto(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)  # Remover URLs\n",
        "    texto = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö√ß√á ]', '', texto)     # Remover caracteres especiais (ajuste para outros idiomas)\n",
        "    return texto\n",
        "\n",
        "texto_normalizado = normalizar_texto(texto)\n",
        "\n",
        "# 2. Tokeniza√ß√£o (nltk)\n",
        "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
        "\n",
        "# 3. Remo√ß√£o de stopwords (nltk)\n",
        "stopwords_pt = set(stopwords.words('portuguese'))\n",
        "tokens_sem_stopwords = [token for token in tokens if token not in stopwords_pt]\n",
        "\n",
        "# 4. Stemming (nltk)\n",
        "stemmer = nltk.RSLPStemmer()\n",
        "tokens_stem = [stemmer.stem(token) for token in tokens_sem_stopwords]\n",
        "\n",
        "# 5. Lematiza√ß√£o (spaCy)\n",
        "def lematizar_com_spacy(tokens):\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "tokens_lematizados = lematizar_com_spacy(tokens_sem_stopwords)\n",
        "\n",
        "# 6. Compara√ß√£o\n",
        "print(\"Texto Original:\\n\", texto)\n",
        "print(\"\\nTexto Normalizado:\\n\", texto_normalizado)\n",
        "print(\"\\nTokens:\\n\", tokens)\n",
        "print(\"\\nTokens Sem Stopwords:\\n\", tokens_sem_stopwords)\n",
        "print(\"\\nStemming:\\n\", tokens_stem)\n",
        "print(\"\\nLematiza√ß√£o:\\n\", tokens_lematizados)"
      ],
      "metadata": {
        "id": "wu3xzhZcyvOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exemplo 03 - O modelo pre treinado"
      ],
      "metadata": {
        "id": "Y0bvOD9Uy8Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Carregar o modelo para portugu√™s\n",
        "nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "# Processar um texto em portugu√™s\n",
        "textoRecebido = input(\"Digite um texto para ser analisado: \")\n",
        "doc = nlp(textoRecebido)\n",
        "\n",
        "print('\\nAn√°lise gramatical das palavras:')\n",
        "for token in doc:\n",
        "    print(f\"Palavra: {token.text}, Classe: {token.pos_}\")\n",
        "\n",
        "print(\"\\nAnalise de Depend√™ncias:\")\n",
        "for token in doc:\n",
        "  print(f\"Palavra: {token.text}, Depende de: {token.head.text}\")\n",
        "\n",
        "# Visualizar a √°rvore graficamente (opcional)\n",
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "P_cj5JK9zGQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* O que s√£o: Conjuntos de dados estat√≠sticos e regras, Treinados com milh√µes de textos, Especializados em tarefas espec√≠ficas de linguagem, Resultado de aprendizado de m√°quina\n",
        "* Como s√£o treinados: Alimentados com grande volume de textos, Aprendem padr√µes do idioma, Reconhecem estruturas gramaticais, Identificam rela√ß√µes entre palavras, S√£o testados e refinados\n",
        "* Tipos de modelos por tamanho:\n",
        "  * Pequeno (sm): Mais r√°pido, menor precis√£o, Usa menos mem√≥ria, Bom para testes\n",
        "  * M√©dio (md): Equil√≠brio entre velocidade e precis√£o, Precis√£o moderada\n",
        "Bom para uso geral\n",
        "  * Grande (lg): Mais preciso, Mais lento, Usa mais mem√≥ria, Melhor para an√°lises detalhadas\n",
        "* Alguns modelos pr√©-treinados:\n",
        "  * Portugu√™s - nlp_pt = spacy.load('pt_core_news_sm')\n",
        "  * Ingl√™s - nlp_en = spacy.load('en_core_web_sm')\n",
        "  * Espanhol - nlp_es = spacy.load('es_core_news_sm')\n",
        "  * Franc√™s - nlp_fr = spacy.load('fr_core_news_sm')\n",
        "  * Alem√£o - nlp_de = spacy.load('de_core_news_sm')"
      ],
      "metadata": {
        "id": "dUcm9tmkzPXg"
      }
    }
  ]
}